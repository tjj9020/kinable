# AI Provider Configuration
# This file defines the configurations for various AI model providers.
# It will be used to populate the ProviderConfigurationTable in DynamoDB.

# The top-level key here represents the 'configId' in the DynamoDB table.
# We've decided to use a globally consistent configId.
GLOBAL_AISERVICE_CONFIG_V1:
  configVersion: "1.0.0" 
  schemaVersion: "1.0.0" 
  updatedAt: "NEEDS UPDATE ON EACH CHANGE" # This will be updated by the load script
  
  providers: # Was 'configuration' in old YAML, maps to AiServiceConfiguration.providers
    openai:
      active: true
      secretId: "{env}-{region}-openai-api-key" # Corrected template
      defaultModel: "gpt-3.5-turbo"
      models:
        "gpt-4o":
          name: "GPT-4 Omni"
          description: "Our most advanced, multimodal model that's more capable, more accurate, and faster than ever before."
          costPerMillionInputTokens: 2.50
          costPerMillionOutputTokens: 10.00
          contextWindow: 128000
          maxOutputTokens: 16384
          capabilities: ["general", "vision", "reasoning", "coding", "chat", "image"]
          streamingSupport: true
          functionCallingSupport: true
          visionSupport: true 
          active: true
        "gpt-4o-mini": # Was "o4-mini" in previous user YAML, changed key for consistency
          name: "GPT-4o mini" # User had "gpt-4o-mini" as name for "o4-mini"
          description: "GPT-4o mini (“o” for “omni”) is a fast, affordable small model for focused tasks. It accepts both text and image inputs, and produces text outputs (including Structured Outputs). It is ideal for fine-tuning, and model outputs from a larger model like GPT-4o can be distilled to GPT-4o-mini to produce similar results at lower cost and latency.."
          costPerMillionInputTokens: 0.15
          costPerMillionOutputTokens: 0.60
          contextWindow: 128000
          maxOutputTokens: 16384
          capabilities: ["general", "reasoning", "coding", "chat", "vision"] # Original YAML's "o4-mini" had vision:false. Description here for "gpt-4o-mini" implies vision.
          streamingSupport: true
          functionCallingSupport: true
          visionSupport: false # Kept based on original "o4-mini" having vision: false
          active: true
        "gpt-3.5-turbo":
          name: "GPT-3.5 Turbo"
          description: "Improved model for speed and cost-effectiveness."
          costPerMillionInputTokens: 0.50
          costPerMillionOutputTokens: 1.50
          contextWindow: 16385 # Max for gpt-3.5-turbo-0125
          maxOutputTokens: 4096
          capabilities: ["general", "chat", "coding"]
          streamingSupport: true
          functionCallingSupport: true
          visionSupport: false
          active: true
      # Add other OpenAI specific configurations if needed

    anthropic:
      active: true
      secretId: "{env}-{region}-anthropic-api-key" # Corrected template
      defaultModel: "claude-3-haiku-20240307"
      models:
        "claude-3-opus-20240229":
          name: "Claude 3 Opus"
          description: "Anthropic's most powerful model, delivering state-of-the-art performance on highly complex tasks and demonstrating flu​​ency and human-like understanding."
          costPerMillionInputTokens: 15.00
          costPerMillionOutputTokens: 75.00
          contextWindow: 200000
          maxOutputTokens: 4096
          capabilities: ["general", "reasoning", "coding", "chat"]
          streamingSupport: true
          functionCallingSupport: false # Anthropic tool use is different
          visionSupport: true 
          active: true
        "claude-3-sonnet-20240229":
          name: "Claude 3 Sonnet"
          description: "An ideal balance of intelligence and speed for enterprise workloads. Maximum utility at a lower price, dependable, high-endurance workloads."
          costPerMillionInputTokens: 3.00
          costPerMillionOutputTokens: 15.00
          contextWindow: 200000
          maxOutputTokens: 4096
          capabilities: ["general", "reasoning", "coding", "chat"]
          streamingSupport: true
          functionCallingSupport: false
          visionSupport: true
          active: true
        "claude-3-haiku-20240307":
          name: "Claude 3 Haiku"
          description: "Anthropic's fastest, most compact model for near-instant responsiveness. Quick and accurate targeted performance."
          costPerMillionInputTokens: 0.25
          costPerMillionOutputTokens: 1.25
          contextWindow: 200000
          maxOutputTokens: 4096
          capabilities: ["general", "reasoning", "coding", "chat"]
          streamingSupport: true
          functionCallingSupport: false
          visionSupport: true
          active: true
      # Add other Anthropic specific configurations if needed
          
  routing:
    # Default routing weights from config-schema.ts
    weights: 
      cost: 0.4
      quality: 0.3
      latency: 0.2
      availability: 0.1
    # Provider preference order - list active providers you generally prefer
    providerPreferenceOrder: ["openai", "anthropic"] 
    # Optional global default model if no other selection logic applies
    # defaultModel: "gpt-3.5-turbo" 
    # rules: [] # Optional: advanced routing rules
    
  featureFlags: {} # Empty object for feature flags, e.g., { "newModelRollout": true }

# Add other providers here in the future, e.g., 'gemini', 'perplexity'
# gemini:
#   secretId: "kinable-{env}/{region}/gemini/api-key"
#   defaultModel: "gemini-1.5-flash"
#   models:
#     - modelId: "gemini-1.5-pro"
#       # ... other gemini model details 